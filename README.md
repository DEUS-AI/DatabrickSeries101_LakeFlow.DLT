# DLT Wikipedia Project

## Overview

This project provides a comprehensive framework for processing real-time Wikipedia edit data using Delta Live Tables (DLT) on Databricks. It demonstrates a robust medallion architecture (Bronze, Silver, Gold) and is structured to support both independent, single-layer pipelines and a unified, end-to-end pipeline.

The primary goal is to showcase best practices in DLT development, including modular pipeline design, data quality enforcement with expectations, and deployment automation using Databricks Asset Bundles.

## Project Structure

The core logic of the project resides in the `DLTs/` directory, which is organized as follows:

```
DLTs/
├── bronze/               # Scripts for the individual Bronze layer pipeline
│   ├── 01_wikipedia_edits_cleaned.py
│   └── 02_wikipedia_edits_summary.py
├── gold/                 # Scripts for the individual Gold layer pipeline
│   ├── 01_wikipedia_editor_insights.py
│   └── 02_wikipedia_page_trends.py
├── landing/              # Script for the individual Landing layer pipeline
│   └── 01_raw_wikipedia_edits.py
├── sequential/           # Scripts for the unified, end-to-end pipeline
│   ├── bronze/
│   │   └── 01_bronze_wikipedia_tables.py
│   ├── gold/
│   │   └── 01_gold_wikipedia_tables.py
│   ├── landing/
│   │   └── 01_raw_wikipedia_edits.py
│   └── silver/
│       └── 01_silver_wikipedia_tables.py
├── silver/               # Scripts for the individual Silver layer pipeline
│   ├── 01_wikipedia_top_editors.py
│   └── 02_wikipedia_top_pages.py
└── unified/
    └── 01_unified_wikipedia_pipeline.py
```

-   **Individual Layer Directories (`landing/`, `bronze/`, etc.):** Each of these folders contains the DLT scripts for a specific, isolated pipeline. These can be run independently for development, testing, or targeted data processing.
-   **`sequential/` Directory:** This directory contains the scripts for a single, unified pipeline that runs all layers in sequence. The tables generated by this pipeline are suffixed with `_seq` to prevent conflicts with the individual layer pipelines.

## Pipelines

This project is configured to run two types of DLT pipelines, both defined in the `databricks.yml` file.

### 1. Individual Layer Pipelines

-   **`01-wikipedia-landing-pipeline`**: Ingests raw JSON data and creates the initial `raw_wikipedia_edits` table.
-   **`02-wikipedia-bronze-pipeline`**: Cleans and structures the raw data, creating the `wikipedia_edits_cleaned` and `wikipedia_edits_summary` tables.
-   **`03-wikipedia-silver-pipeline`**: Aggregates the bronze data to create business-level tables like `wikipedia_top_editors` and `wikipedia_top_pages`.
-   **`04-wikipedia-gold-pipeline`**: Builds on the silver tables to create final, enriched tables for analytics, such as `wikipedia_editor_insights` and `wikipedia_page_trends`.

These pipelines read from the tables created by the preceding layer (e.g., the bronze pipeline reads from the `landing` schema).

### 2. Unified End-to-End Pipeline

-   **`05-wikipedia-unified-pipeline`**: This pipeline is configured to run all the scripts in the `DLTs/sequential/` directory. It provides a complete, end-to-end view of the data flow graph from landing to gold.
-   **Table Naming:** To avoid conflicts with the individual pipelines, all tables created by the unified pipeline have a `_seq` suffix (e.g., `raw_wikipedia_edits_seq`).

## Data Quality

Data integrity is enforced throughout the pipelines using DLT Expectations. These are data quality checks that validate the data as it's being processed.

-   **`expect_or_drop`**: Used in the landing and bronze layers to discard records that are missing essential information (e.g., a null `id` or `editor`).
-   **`expect`**: Used in the gold layer to issue warnings about data that might be unusual but doesn't need to be dropped (e.g., a negative `activity_span_days` for an editor).

These expectations help ensure that the data is reliable and that any quality issues are caught early.

## How to Run

This project is configured as a Databricks Asset Bundle, which simplifies deployment and management.

### Prerequisites

-   [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html) installed and configured.
-   Access to a Databricks workspace.

### Deployment

1.  **Validate the Bundle:**
    Open a terminal in the project's root directory and run:
    ```bash
    databricks bundle validate
    ```

2.  **Deploy the Pipelines:**
    Deploy all the pipelines to your Databricks workspace using:
    ```bash
    databricks bundle deploy
    ```
    This command reads the `databricks.yml` file and creates or updates the DLT pipelines in your workspace.

3.  **Run the Pipelines:**
    Once deployed, you can run the pipelines from the Databricks UI under **Workflows > Delta Live Tables**. You can choose to run the individual layer pipelines or the complete `05-wikipedia-unified-pipeline`.
